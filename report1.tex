
\def\problemset#1#2{
\begin{center}
\framebox{\framebox{\vspace{0.5in}
  \parbox{12cm}{\bf
    Gaurav Nanda \hfill Homework \#1\\
    Natural Language Processing \hfill #2, 2014}
  }}\vspace{0.05in}
\end{center}
}

\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{footnote}

\setlength{\parindent}{0in}
\setlength{\parskip}{2mm}

%\setlength{\oddsidemargin}{-0.5cm}
%\setlength{\evensidemargin}{-0.5cm}
%\setlength{\textwidth}{18cm}

%\setlength{\topmargin}{-1.7cm}
%\setlength{\textheight}{25cm}
\begin{document}
\problemset{1}{February 16}

\section{Forward Bigram vs Backgram Bigram Model}

\subsection{Backward Training Approaches}

 There are two approaches that could have been used to train the Backgram Bigram Model.

1. Train sentences in the same order as the Forward Bigram Model, i.e. pick the first sentence and keep on moving forward till the end. However, while training individual sentence, read tokens in the reverse order.

2. Train sentences in the reverse order, i.e. pick the last sentence first and keep on going back till the first sentence. Also while training individual sentence, read tokens in the reverse order.

Intuitively, both of these approaches should yield the same result. However, there is a difference due to the approach we follow in handling the Out of Vocabulary words. Whenever, we encounter a new word, we replace that with $<$UNK$>$. Going left to right (L2R) or R2L, would not impact unigram probabilities, but would make a difference to the bigram probabilities.

I ran some tests and found that difference in word perplexity calculated by these two approaches in very less ($<$0.5\%). {\bfseries {Therefore, I just decided to go with the simpler, first approach.}}

\subsection{Implementation and Results - Word Perplexity}

To compute word perplexity in the backward model, we simply reverse each sentence and calculate the sentence log probability using similar scheme as forward model. 

While interpolating, we are using 10\% unigram and 90\% bigram contributions for both forward and backward models.

\begin{center}	
    \begin{table}[ht]
    \centering
    \begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    & \multicolumn{2}{ |c| }{ATIS}  &  \multicolumn{2}{ |c| }{WSJ} &  \multicolumn{2}{ |c| }{BROWN}   \\ \hline
    & Train & Test & Train & Test & Train & Test \\ \hline
    Forward Model &  10.59 & 24.05 & 88.89  & 275.11  &  113.35 &  310.66 \\ \hline
    Backward Model &  11.63 & 27.16  & 86.66  &  266.35 & 110.78  & 299.68 \\ \hline
    \end{tabular}
    \caption{Word perplexity for Forward and Backward Bigram Models.}
    \end{table}%
\end{center}

\subsection{Analysis:}
Our results show that Backward model is slightly better for WSJ and Brown datasets. However, forward model is slightly better for ATIS dataset.

These results were contrary to my expectations. I read this\cite{Bangla} paper, which talks about Bangla language giving better results with the backward model, because of the language structure. This led me thinking that English is syntactically ordered from left to right, therefore forward model should be atleast as good as the backward model.

Investigating furhter, I calculated the count of unique tokens in the beginning and end of each sentence for all three datasets and found very interesting results.

\begin{center}	
    \begin{table}[ht]
    \centering
    \begin{tabular}{| l | l | l | l |}
    \hline
     & ATIS &  WSJ & BROWN \\ \hline
     Initial Unique Tokens &  48 & 2899 & 2801 \\ \hline
     Termination Unique Tokens & 120 & 256 & 8 \\ \hline
    \end{tabular}
    \caption{Count of unique start and termination tokens in each corpus.}
    \end{table}%
\end{center}

{\bfseries Hypothesis:} Datasets where number of unique termination tokens are lesser than starting unique tokens, Backward bigram model seems to perform better than Forward Backward model and vice-versa.

We also observe that for {\bfseries Brown} corpus, all the sentence ending tokens seem to be one among some 8 tokens. Here is table of those tokens with their associated bigram probabilities $p(W|</S>)$:

\begin{center}	
    \begin{table}[ht]
    \centering
    \begin{tabular}{| l | l | }
    \hline
    Word & Probability \\ \hline
   $</S><UNK>$ & 6.35E-5\\ \hline
   $</S>Governor$ & 2.11E-5 \\ \hline
   $</S>.$ & 0.94 \\ \hline
   $</S>3$ & 2.11E-5  \\ \hline
   $</S>?$ & 0.04  \\ \hline 
   $</S>!$ & 0.0125  \\ \hline 
   $</S>..$ & 0.0015  \\ \hline 
   $</S>insulated$ & 2.11E-5  \\ \hline 
    \end{tabular}
    \caption{All the sentence termination tokens in Brown corpus with their bigram probabilities.}
    \end{table}%
\end{center}

This table suggests that, most of the sentences in the corpus are ending with "." and we are considering DOT as a separate token. Therefore, when we test our backward Bigram model on the test data, which also has DOT as the termination token for most of the sentences, it tends to assign high probability for DOT and overall probability of the sentence goes high. 

On the other hand, in the forward model, DOT comes in the end of the sentence and DOT can follow a number of tokens, therfore its prediction probability would be lesser.

To test my hypothesis, I removed dots from the training and test data and results were as follows:

\begin{center}	
    \begin{table}[ht]
    \centering
    \begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    & \multicolumn{2}{ |c| }{ATIS}  &  \multicolumn{2}{ |c| }{WSJ} &  \multicolumn{2}{ |c| }{BROWN}   \\ \hline
    & Train & Test & Train & Test & Train & Test \\ \hline
    Forward Model &  10.59 & 24.05 & 98.11  & 311.38  &  126.26 &  363.22 \\ \hline
    Backward Model &  11.63 & 27.16  & 104.01  &  331.80 & 134.75  & 396.78 \\ \hline
  \end{tabular}
    \caption{Word perplexity for Forward and Backward Bigram Models after removing dots.}
    \end{table}%
\end{center}

\begin{center}	
    \begin{table}[ht]
    \centering
    \begin{tabular}{| l | l | l | l |}
    \hline
     & ATIS &  WSJ & BROWN \\ \hline
     Initial Unique Tokens &  48 & 2899 & 2801 \\ \hline
     Termination Unique Tokens & 120 & 7085 & 9490 \\ \hline
    \end{tabular}
    \caption{Count of unique start and termination tokens in each corpus after removing dots.}
    \end{table}%
\end{center}

For ATIS dataset, results are unchanged because there are no dots present at the end of sentences. However, for BROWN  and WSJ corpus, there is drastic change in numbers. Now, the number of unique termination tokens have increased to a large extent. Therefore, the word perplexity of the backward model is higher (performance is worse) in comparison to forward model. These results support our initial hypothesis.

\subsection{Code Execution:}

I have added another option to the code to remove dots "." from sentences. 

{\bfseries Sample Execution:}

// If you do not want to remove dots.
\newline
{\bfseries java nlp.lm.BackwardBigramModel ./pos/atis/ 0.1 false}


// If you want to remove "." (dots).
\newline
{\bfseries java nlp.lm.BackwardBigramModel ./pos/atis/ 0.1 true}

\subsection{Conclusion: }

If we want to treat tokens like ".", "?", "!" etc as separate tokens, backward model seem to be behaving little better than the forward model. However, if we remove the sentence termination tokens, then forward model seems to be little better than the backward bigram model.

Overall, performance difference in forward and backward models is not huge.

\section {Bidirectional Model}

\subsection {Implementation}

This is implemented using already existing Forward and Backward Bigram models. Initially, both forward and backward models are trained individually. While testing, for each sentence we compute the bigram probabilities of every token using both forward and backward model. Then we find the final bigram probabilities by using forward and backward numbers in a fixed ratio.

We have to little careful in this implementation because array of probabilities returned by Backward Bigram model is in the reverse order. Therefore, we have to properly handle array indices to ensure that we are picking up the corresponding numbers from two arrays.

\subsection{Results:}

In the basic approach, we have taken contribution of both forward and backward model to be same 50%.

\begin{center}	
    \begin{table}[ht]
    \centering
    \begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    & \multicolumn{2}{ |c| }{ATIS}  &  \multicolumn{2}{ |c| }{WSJ} &  \multicolumn{2}{ |c| }{BROWN}   \\ \hline
    & Train & Test & Train & Test & Train & Test \\ \hline
    Forward Model &  10.59 & 24.05 & 88.89  & 275.11  &  113.35 &  310.66 \\ \hline
    Backward Model &  11.63 & 27.16  & 86.66  &  266.35 & 110.78  & 299.68 \\ \hline
    Bidirectional Model &  7.23 & 12.7  & 46.51 &  126.11 & 61.46  & 167.48 \\ \hline
    \end{tabular}
    \caption{Word perplexity for Forward and Backward Bigram Models.}
    \end{table}%
\end{center}

\subsection {Analysis}

Using bidirectional model, we see a huge increase in the performance. Primary reason for the increase is that we are looking both forward and backward of a token before assigning it a probability. Forward and backward information gives more context about the token and hence higher probability of predicting the token in a similar context.  This is somehat simile to training your model over higher order models like trigram or 4-gram models.

Currently we are using (token-1 and token+1) to calculate probability of a token. I surmise a model using (token-2, token-1, token+1, token+2) would give even better performance. However, there is also a chance of overfitting with increasing order of bidirectional n-gram models.

Another issue is to find the optimal ratio of forward and backward models to be used. Word perplexity numbers computed by Forward and Backward are almost similar, therefore optimal ratio is also very near to 50\%. However, results are little skewed towards the model which has lesser value of perplexity. 

For example, for Brown corpus, 

A) If we don't remove Dots from the model, we expect the optimal number to little baised towards backward. 

\begin{center}	
    \begin{table}[ht]
    \centering
    \begin{tabular}{| l | l | l |}
    \hline
    Forward Ratio  &  Word Perplexity (Training) & Word Perplexity (Test)   \\ \hline
0 &	110.78 &	299.68 \\ \hline
0.2 & 	67.01 & 182.24  \\ \hline
0.4 & 	61.85 & 	168.31 \\ \hline
{\bfseries 0.48} & 	{\bfseries 61.45}	 & {\bfseries 167.39}\\ \hline
0.5	 & 61.46 & 	167.48 \\ \hline
0.52 & 	61.52 & 	167.7 \\ \hline
0.6 & 	62.22 & 	169.86 \\ \hline
0.8 & 	68.26 & 	187.5 \\ \hline
1 & 	113.35 & 	310.66 \\ \hline
    \end{tabular}
    \caption{Word perplexity for different ratios of Forward and Backward model}
    \end{table}%
\end{center}

B) If we do remove Dots from the model, we expect the optimal number to little baised towards forward. 

\begin{center}	
    \begin{table}[ht]
    \centering
    \begin{tabular}{| l | l | l |}
    \hline
    Forward Ratio  &  Word Perplexity (Training) & Word Perplexity (Test)   \\ \hline
0 &	134.74  & 396.78 \\ \hline
0.2	& 79.15 &	233.88 \\ \hline
0.4 & 72.11 &	212.56 \\ \hline
0.48	 & 71.3	 & 210.12 \\ \hline
0.5 & 71.23 &	209.91 \\ \hline
{\bfseries 0.52} & {\bfseries 71.21} & {\bfseries 209.86}\\ \hline
0.6	 & 71.66	 & 211.2\\ \hline
0.8 & 	77.52	 & 228.73\\ \hline
1	 & 126.26	 & 363.22\\ \hline
    \end{tabular}
    \caption{Word perplexity for different ratios of Forward and Backward model}
    \end{table}%
\end{center}

\begin{thebibliography}{100} % 100 is a random guess of the total number of 
%references 

\bibitem{Bangla} http://www.bracuniversity.net/research/crblp/papers/BAN04.pdf)

\end{thebibliography} 
\end{document}